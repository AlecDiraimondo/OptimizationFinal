{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some info about this notebook:\n",
    "- Currently using resnet20 or resnet32 for cifar10, if you want to use other network, feel free to change it in the `train` function as that's where downloading the dataset is\n",
    "- `depth` hyperparam in `general_hyperparameters` will allow toggle between resnet20 or 32\n",
    "- Currently using `MultiStepLR` as learning rate scheduler\n",
    "- Feel free to go through the `train` function as change as you see fit to add Apollo in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "# sys.path.append(\"C:\\\\Users\\\\Tuan Tran\\\\Desktop\\\\CS595_final\\\\adahessian\\\\image_classification\")\n",
    "\n",
    "import numpy as np\n",
    "import argparse\n",
    "from tqdm import tqdm, tqdm_notebook, trange\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from optim.adahessian_helpers.utils import *\n",
    "from optim.adahessian_helpers.models.resnet import *\n",
    "from optim.adahessian_helpers.optim_adahessian import Adahessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to initialize optimizers, download data, lr scheduler then training loop and checkpoint best model\n",
    "\"\"\"\n",
    "def train(dataset_name,\n",
    "          optimizer_str,\n",
    "          lr,\n",
    "          lr_decay,\n",
    "          lr_decay_epoch,\n",
    "          weight_decay,\n",
    "          batch_size,\n",
    "          test_batch_size,\n",
    "          epochs,\n",
    "          depth,\n",
    "          seed):\n",
    "\n",
    "    # set random seed to reproduce the work\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "    # Create dir for checkpoint best model per optimizer\n",
    "    if not os.path.isdir('checkpoint/'):\n",
    "        os.makedirs('checkpoint/')\n",
    "    \n",
    "    print(f\"Retrieving {dataset_name}\")\n",
    "    # get dataset\n",
    "    train_loader, test_loader = getData(\n",
    "        name=dataset_name, train_bs=batch_size, test_bs=test_batch_size)\n",
    "\n",
    "    # make sure to use cudnn.benchmark for second backprop\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    # get model and optimizer\n",
    "    model = resnet(num_classes=10, depth=depth).cuda()\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    #print('    Total params: %.2fM' % (sum(p.numel()\n",
    "    #                                       for p in model.parameters()) / 1000000.0))\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if optimizer_str == 'sgd':\n",
    "        optimizer = optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=lr,\n",
    "            momentum=0.9,\n",
    "            weight_decay=weight_decay)\n",
    "    elif optimizer_str == 'adam':\n",
    "        optimizer = optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=lr,\n",
    "            weight_decay=weight_decay)\n",
    "    elif optimizer_str == 'adamw':\n",
    "        print('For AdamW, we automatically correct the weight decay term for you! If this is not what you want, please modify the code!')\n",
    "        weight_decay = weight_decay / lr\n",
    "        optimizer = optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=lr,\n",
    "            weight_decay=weight_decay)\n",
    "    elif optimizer_str == 'adahessian':\n",
    "        print('For AdaHessian, we use the decoupled weight decay as AdamW. Here we automatically correct this for you! If this is not what you want, please modify the code!')\n",
    "        weight_decay = weight_decay / lr\n",
    "        optimizer = Adahessian(\n",
    "            model.parameters(),\n",
    "            lr=lr,\n",
    "            weight_decay=weight_decay)\n",
    "    else:\n",
    "        raise Exception('We do not support this optimizer yet!!')\n",
    "\n",
    "    # learning rate schedule\n",
    "    scheduler = lr_scheduler.MultiStepLR(\n",
    "        optimizer,\n",
    "        lr_decay_epoch,\n",
    "        gamma=lr_decay,\n",
    "        last_epoch=-1)\n",
    "\n",
    "    # Start training loop\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print('Current Epoch: ', epoch)\n",
    "        train_loss = 0.\n",
    "        total_num = 0\n",
    "        correct = 0\n",
    "\n",
    "        scheduler.step()\n",
    "        model.train()\n",
    "        with tqdm_notebook(total=len(train_loader.dataset)) as progressbar:\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward(create_graph=True)\n",
    "                train_loss += loss.item() * target.size()[0]\n",
    "                total_num += target.size()[0]\n",
    "                _, predicted = output.max(1)\n",
    "                correct += predicted.eq(target).sum().item()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                progressbar.update(target.size(0))\n",
    "\n",
    "        acc = test(model, test_loader)\n",
    "        train_loss /= total_num\n",
    "        #print(f\"Training Loss of Epoch {epoch}: {np.around(train_loss, 2)}\")\n",
    "        #print(f\"Testing of Epoch {epoch}: {np.around(acc * 100, 2)}\")\n",
    "\n",
    "        if acc > best_acc:\n",
    "            # Only display if found new best model\n",
    "            print(f\"Training Loss of Epoch {epoch}: {np.around(train_loss, 2)}\")\n",
    "            print(f\"Testing of Epoch {epoch}: {np.around(acc * 100, 2)}\")\n",
    "            best_acc = acc\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'best_accuracy': best_acc,\n",
    "                }, f'checkpoint/{optimizer_str}_netbest.pkl')\n",
    "    \n",
    "    print(f'Best Acc for {optimizer_str} with dataset {dataset_name}: {np.around(best_acc * 100, 2)}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADAHESSIAN\n",
      "Retrieving cifar10\n",
      "Files already downloaded and verified\n",
      "For AdaHessian, we use the decoupled weight decay as AdamW. Here we automatically correct this for you! If this is not what you want, please modify the code!\n",
      "Current Epoch:  1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a85f88fdcb84a9cba73e06d01fb3c67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Loss of Epoch 1: 2.18\n",
      "Testing of Epoch 1: 36.83\n",
      "Current Epoch:  2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f8fc3b090aa4f75916ec40000d49f3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-73a5dde07d52>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{optimizer_str.upper()}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[0moptim_hyperparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim_to_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moptimizer_str\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moptim_hyperparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mgeneral_hyperparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-215f92c642ec>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(dataset_name, optimizer_str, lr, lr_decay, lr_decay_epoch, weight_decay, batch_size, test_batch_size, epochs, depth, seed)\u001b[0m\n\u001b[0;32m     92\u001b[0m                 \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m                 \u001b[0mcorrect\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m                 \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m                 \u001b[0mprogressbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m                 \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m             \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_with_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\CS595_final\\OptimizationFinal\\optim\\adahessian_helpers\\optim_adahessian.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[1;31m# get the Hessian diagonal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m         \u001b[0mhut_traces\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhut_trace\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhut_traces\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\CS595_final\\OptimizationFinal\\optim\\adahessian_helpers\\optim_adahessian.py\u001b[0m in \u001b[0;36mget_trace\u001b[1;34m(self, params, grads)\u001b[0m\n\u001b[0;32m     81\u001b[0m             \u001b[0mgrad_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[0monly_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m             retain_graph=True)\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0mhutchinson_trace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[0;32m    147\u001b[0m     return Variable._execution_engine.run_backward(\n\u001b[0;32m    148\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m         inputs, allow_unused)\n\u001b[0m\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Training settings\n",
    "\"\"\"\n",
    "# Specific hyperparams for each optimizer\n",
    "optim_to_params = {\n",
    "    \"sgd\": {\n",
    "        \"lr\": 0.1,\n",
    "        'lr_decay': 0.1,\n",
    "        'lr_decay_epoch': [80, 120],\n",
    "        'weight_decay': 5e-4\n",
    "    },\n",
    "    \"adam\": {\n",
    "        \"lr\": 0.001,\n",
    "        'lr_decay': 0.1,\n",
    "        'lr_decay_epoch': [80, 120],\n",
    "        'weight_decay': 5e-4\n",
    "    },\n",
    "    \"adamw\": {\n",
    "        \"lr\": 0.01,\n",
    "        'lr_decay': 0.1,\n",
    "        'lr_decay_epoch': [80, 120],\n",
    "        'weight_decay': 5e-4\n",
    "    },\n",
    "    \"adahessian\": {\n",
    "        \"lr\": 0.15,\n",
    "        'lr_decay': 0.1,\n",
    "        'lr_decay_epoch': [80, 120],\n",
    "        'weight_decay': 5e-4\n",
    "    }\n",
    "}\n",
    "\n",
    "# General hyperparameters that apply to all optimizers, feel free to change this around as you see fit\n",
    "general_hyperparams = {\n",
    "    'batch_size': 256,\n",
    "    'test_batch_size': 256,\n",
    "    'epochs': 160,\n",
    "    # Depth of ResNet, 20 = resnet20, 32 = resnet32\n",
    "    'depth': 20,\n",
    "    'seed': 1\n",
    "}\n",
    "\n",
    "# cifar10 or mnist\n",
    "dataset_name = \"cifar10\"\n",
    "\n",
    "optimizers = ['adahessian']\n",
    "# Run the loop to train using each optimizer in the list of optimizers\n",
    "for optimizer_str in optimizers:\n",
    "    print(f\"{optimizer_str.upper()}\")\n",
    "    optim_hyperparams = optim_to_params[optimizer_str]\n",
    "    train(dataset_name, optimizer_str, **optim_hyperparams, **general_hyperparams)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
