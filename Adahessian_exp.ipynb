{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some info about this notebook:\n",
    "- Currently using resnet20 or resnet32 for cifar10, if you want to use other network, feel free to change it in the `train` function as that's where downloading the dataset is\n",
    "- `depth` hyperparam in `general_hyperparameters` will allow toggle between resnet20 or 32\n",
    "- Currently using `MultiStepLR` as learning rate scheduler\n",
    "- Feel free to go through the `train` function as change as you see fit to add Apollo in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "# sys.path.append(\"C:\\\\Users\\\\Tuan Tran\\\\Desktop\\\\CS595_final\\\\adahessian\\\\image_classification\")\n",
    "\n",
    "import numpy as np\n",
    "import argparse\n",
    "from tqdm import tqdm, tqdm_notebook, trange\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from optim.adahessian_helpers.utils import *\n",
    "from optim.adahessian_helpers.models.resnet import *\n",
    "from optim.adahessian_helpers.optim_adahessian import Adahessian\n",
    "from optim import Apollo\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to initialize optimizers, download data, lr scheduler then training loop and checkpoint best model\n",
    "\"\"\"\n",
    "def train(dataset_name,\n",
    "          optimizer_str,\n",
    "          lr,\n",
    "          lr_decay,\n",
    "          lr_decay_epoch,\n",
    "          weight_decay,\n",
    "          batch_size,\n",
    "          test_batch_size,\n",
    "          epochs,\n",
    "          depth,\n",
    "          seed):\n",
    "\n",
    "    # set random seed to reproduce the work\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "    # Create dir for checkpoint best model per optimizer\n",
    "    if not os.path.isdir('checkpoint/'):\n",
    "        os.makedirs('checkpoint/')\n",
    "    \n",
    "    print(f\"Retrieving {dataset_name}\")\n",
    "    # get dataset\n",
    "    train_loader, test_loader = getData(\n",
    "        name=dataset_name, train_bs=batch_size, test_bs=test_batch_size)\n",
    "\n",
    "    # make sure to use cudnn.benchmark for second backprop\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    # get model and optimizer\n",
    "    model = resnet(num_classes=10, depth=depth).cuda()\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    #print('    Total params: %.2fM' % (sum(p.numel()\n",
    "    #                                       for p in model.parameters()) / 1000000.0))\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if optimizer_str == 'sgd':\n",
    "        optimizer = optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=lr,\n",
    "            momentum=0.9,\n",
    "            weight_decay=weight_decay)\n",
    "    elif optimizer_str == 'adam':\n",
    "        optimizer = optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=lr,\n",
    "            weight_decay=weight_decay)\n",
    "    elif optimizer_str == 'adamw':\n",
    "        print('For AdamW, we automatically correct the weight decay term for you! If this is not what you want, please modify the code!')\n",
    "        weight_decay = weight_decay / lr\n",
    "        optimizer = optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=lr,\n",
    "            weight_decay=weight_decay)\n",
    "    elif optimizer_str == 'adahessian':\n",
    "        print('For AdaHessian, we use the decoupled weight decay as AdamW. Here we automatically correct this for you! If this is not what you want, please modify the code!')\n",
    "        weight_decay = weight_decay / lr\n",
    "        optimizer = Adahessian(\n",
    "            model.parameters(),\n",
    "            lr=lr,\n",
    "            weight_decay=weight_decay)\n",
    "    elif optimizer_str == \"apollo\":\n",
    "        weight_decay = weight_decay / lr \n",
    "        optimizer = Apollo(\n",
    "            model.parameters(),\n",
    "            lr = lr,\n",
    "            weight_decay = weight_decay,\n",
    "            init_lr = lr / 5)\n",
    "    else:\n",
    "        raise Exception('We do not support this optimizer yet!!')\n",
    "\n",
    "    # learning rate schedule\n",
    "    scheduler = lr_scheduler.MultiStepLR(\n",
    "        optimizer,\n",
    "        lr_decay_epoch,\n",
    "        gamma=lr_decay,\n",
    "        last_epoch=-1)\n",
    "\n",
    "    # Start training loop\n",
    "    best_acc = 0.0\n",
    "    losses_list = []\n",
    "    acc_list = []\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print('Current Epoch: ', epoch)\n",
    "        train_loss = 0.\n",
    "        total_num = 0\n",
    "        correct = 0\n",
    "\n",
    "        scheduler.step()\n",
    "        model.train()\n",
    "        with tqdm_notebook(total=len(train_loader.dataset)) as progressbar:\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward(create_graph=True)\n",
    "                train_loss += loss.item() * target.size()[0]\n",
    "                total_num += target.size()[0]\n",
    "                _, predicted = output.max(1)\n",
    "                correct += predicted.eq(target).sum().item()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                progressbar.update(target.size(0))\n",
    "\n",
    "        acc = test(model, test_loader)\n",
    "        train_loss /= total_num\n",
    "        #print(f\"Training Loss of Epoch {epoch}: {np.around(train_loss, 2)}\")\n",
    "        #print(f\"Testing of Epoch {epoch}: {np.around(acc * 100, 2)}\")\n",
    "        losses_list.append(train_loss)\n",
    "        acc_list.append(acc)\n",
    "        \n",
    "        if acc > best_acc:\n",
    "            # Only display if found new best model\n",
    "            print(f\"Training Loss of Epoch {epoch}: {np.around(train_loss, 2)}\")\n",
    "            print(f\"Testing of Epoch {epoch}: {np.around(acc * 100, 2)}\")\n",
    "            best_acc = acc\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'best_accuracy': best_acc,\n",
    "                }, f'checkpoint/{optimizer_str}_netbest.pkl')\n",
    "    \n",
    "    print(f'Best Acc for {optimizer_str} with dataset {dataset_name}: {np.around(best_acc * 100, 2)}')\n",
    "    losses_plot_string = (\"%s_%s_loss_plot.jpg\" % (optimizer_str, dataset_name))\n",
    "    accuracy_plot_string = (\"%s_%s_accuracy_plot.jpg\" % (optimizer_str, dataset_name))\n",
    "    #losses plot\n",
    "    loss_fig = plt.figure() \n",
    "    plt.plot(losses_list)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    loss_fig.suptitle(\"%s: Losses Vs Epoch\" % (optimizer_str.upper()))\n",
    "    plt.show()\n",
    "    loss_fig.savefig(losses_plot_string)\n",
    "    #accuracy plot\n",
    "    acc_fig = plt.figure()\n",
    "    plt.plot(acc_list)\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    acc_fig.suptitle(\"%s: Accuracy Vs Epoch\" % (optimizer_str.upper()))\n",
    "    plt.show()\n",
    "    acc_fig.savefig(accuracy_plot_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APOLLO\n",
      "Retrieving cifar10\n",
      "Files already downloaded and verified\n",
      "Current Epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/life/.local/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "/home/life/.local/lib/python3.6/site-packages/ipykernel_launcher.py:94: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daedfcba6c76476684d310472040279e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=50000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Training settings\n",
    "\"\"\"\n",
    "# Specific hyperparams for each optimizer\n",
    "optim_to_params = {\n",
    "    \"sgd\": {\n",
    "        \"lr\": 0.1,\n",
    "        'lr_decay': 0.1,\n",
    "        'lr_decay_epoch': [80, 120],\n",
    "        'weight_decay': 5e-4\n",
    "    },\n",
    "    \"adam\": {\n",
    "        \"lr\": 0.001,\n",
    "        'lr_decay': 0.1,\n",
    "        'lr_decay_epoch': [80, 120],\n",
    "        'weight_decay': 5e-4\n",
    "    },\n",
    "    \"adamw\": {\n",
    "        \"lr\": 0.01,\n",
    "        'lr_decay': 0.1,\n",
    "        'lr_decay_epoch': [80, 120],\n",
    "        'weight_decay': 5e-4\n",
    "    },\n",
    "    \"adahessian\": {\n",
    "        \"lr\": 0.15,\n",
    "        'lr_decay': 0.1,\n",
    "        'lr_decay_epoch': [80, 120],\n",
    "        'weight_decay': 5e-4\n",
    "    },\n",
    "    \"apollo\": {\n",
    "        \"lr\": 0.375,\n",
    "        'lr_decay': 0.1,\n",
    "        'lr_decay_epoch': [80, 120],\n",
    "        'weight_decay': 5e-4\n",
    "    }    \n",
    "}\n",
    "\n",
    "# General hyperparameters that apply to all optimizers, feel free to change this around as you see fit\n",
    "general_hyperparams = {\n",
    "    'batch_size': 256,\n",
    "    'test_batch_size': 256,\n",
    "    'epochs': 5,\n",
    "    # Depth of ResNet, 20 = resnet20, 32 = resnet32\n",
    "    'depth': 32,\n",
    "    'seed': 1\n",
    "}\n",
    "\n",
    "# cifar10 or mnist\n",
    "dataset_name = \"cifar10\"\n",
    "\n",
    "optimizers = ['apollo', 'adahessian']\n",
    "# Run the loop to train using each optimizer in the list of optimizers\n",
    "for optimizer_str in optimizers:\n",
    "    print(f\"{optimizer_str.upper()}\")\n",
    "    optim_hyperparams = optim_to_params[optimizer_str]\n",
    "    train(dataset_name, optimizer_str, **optim_hyperparams, **general_hyperparams)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
